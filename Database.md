# Database

1. **Database**: A structured collection of data organized for efficient retrieval, storage, and management.

2. **Data**: Raw facts, figures, and information that can be stored and processed within a database.

3. **Table**: A fundamental database object that organizes data into rows and columns. Tables are used to represent entities or concepts.

4. **Row**: A horizontal entry in a database table, also known as a record, containing a set of related data.

5. **Column**: A vertical entry in a database table, also known as a field, representing a specific attribute or piece of data.

6. **Primary Key**: A unique identifier for each row in a database table. It ensures data integrity and allows for efficient data retrieval.

7. **Foreign Key**: A column in a table that establishes a link to the primary key of another table, creating relationships between tables in a relational database.

8. **Relational Database**: A type of database that uses structured query language (SQL) to manage and manipulate data stored in tables with defined relationships.

9. **NoSQL Database**: A type of database that does not rely on a fixed schema and is designed for handling unstructured or semi-structured data. It is often used for big data and real-time applications.

10. **SQL (Structured Query Language)**: A domain-specific language used for managing and querying relational databases. It allows users to retrieve, insert, update, and delete data.

11. **Query**: A request made in SQL to extract specific data from a database. Queries are used to filter, sort, and transform data.

12. **Index**: A database structure that enhances the speed of data retrieval by providing a quick lookup mechanism for specific columns in a table.

13. **Normalization**: The process of organizing data in a relational database to minimize data redundancy and improve data integrity.

14. **ACID (Atomicity, Consistency, Isolation, Durability)**: A set of properties that guarantee reliable processing of database transactions. It ensures that database operations are reliable and consistent.

15. **Backup**: A copy of a database made at a specific point in time to protect against data loss or corruption.

16. **Data Warehouse**: A repository for storing and managing large volumes of historical and analytical data, often used for business intelligence and reporting.

17. **Data Mining**: The process of discovering patterns, trends, and insights from large datasets stored in databases.

18. **Schema**: A blueprint or structure that defines the organization of tables, columns, and relationships within a database.

19. **DBMS (Database Management System)**: Software that provides tools for creating, managing, and interacting with databases. Examples include MySQL, Oracle, and Microsoft SQL Server.

20. **Backup and Recovery**: Strategies and processes for regularly creating backups and restoring data in case of data loss or system failures.

21. **Replication**: The process of duplicating and maintaining a synchronized copy of a database on multiple servers to enhance data availability and reliability.

22. **Stored Procedure**: A precompiled and reusable set of SQL statements stored in the database and executed as a single unit.

23. **Transaction**: A sequence of one or more database operations treated as a single unit. It ensures that either all operations succeed or none do.

24. **Concurrency Control**: Techniques used to manage access to a database by multiple users or processes simultaneously to prevent data inconsistency.

25. **Database Administrator (DBA)**: An individual responsible for designing, configuring, and managing databases, including security, performance optimization, and data backup.

26. **Data Migration**: The process of moving data from one database system or environment to another, often involving data transformation and validation.

27. **ETL (Extract, Transform, Load)**: A process used to extract data from various sources, transform it into a common format, and load it into a data warehouse or database for analysis.

28. **Big Data**: Extremely large and complex datasets that cannot be effectively managed or analyzed using traditional database systems.

29. **Data Dictionary**: A repository that contains metadata about the structure and definitions of data within a database.

30. **Database Cluster**: A group of interconnected database servers that work together to provide high availability and scalability for handling large workloads.

31. **Data Integrity**: The accuracy and consistency of data in a database, maintained through constraints, validation rules, and referential integrity.

32. **Data Warehousing**: The process of collecting, storing, and managing data from various sources into a centralized repository, often for analytical and reporting purposes.

33. **Data Modeling**: The process of creating a visual representation (data model) of the structure and relationships of data within a database, often using diagrams like Entity-Relationship Diagrams (ERD).

34. **Backup and Recovery Plan**: A documented strategy outlining how data backups will be performed, how often they will occur, and the procedures for restoring data in case of disasters or failures.

35. **Data Replication**: The process of copying and synchronizing data between multiple databases or servers to ensure high availability and fault tolerance.

36. **Data Warehouse Schema**: The structure used to organize data within a data warehouse, such as star schema, snowflake schema, or galaxy schema.

37. **Triggers**: Special types of stored procedures in a database that are automatically executed when a specific event or condition occurs, such as data modification.

38. **Data Dictionary**: A repository that contains metadata about the structure and definitions of data within a database.

39. **Normalization**: The process of organizing data in a relational database to minimize data redundancy and improve data integrity.

40. **Denormalization**: The opposite of normalization, denormalization involves intentionally introducing redundancy into a database design to improve query performance.

41. **Sharding**: A database scaling technique where data is distributed across multiple servers or shards, allowing for horizontal scaling and improved performance.

42. **Object-Relational Database (ORD)**: A database system that combines features of both relational databases and object-oriented databases, allowing for more complex data types and relationships.

43. **In-memory Database**: A database system that stores data primarily in RAM, enabling faster data retrieval but typically requiring larger memory resources.

44. **Data Warehouse ETL Tools**: Software tools designed to extract, transform, and load data from various sources into a data warehouse, automating the ETL process.

45. **Data Mart**: A subset of a data warehouse that is designed for a specific department or business function, containing data relevant to a particular area of the organization.

46. **CAP Theorem (Consistency, Availability, Partition Tolerance)**: A theoretical framework that explains the trade-offs between the three key properties in distributed systems and databases.

47. **Polyglot Persistence**: The practice of using multiple types of databases (e.g., relational, NoSQL) to handle different data storage and processing needs within an application.

48. **ACID 2.0**: An extended version of the traditional ACID properties that also considers the needs of distributed and cloud-based databases.

49. **Graph Database**: A type of NoSQL database designed to represent and query data as graphs, making it suitable for applications with complex relationships.

50. **Blockchain**: A decentralized and distributed ledger technology that uses cryptographic techniques to ensure data immutability and trust without the need for a central authority.

51. **Spatial Database**: A type of database designed to store and query spatial data, such as geographic information system (GIS) data, for applications like maps and location-based services.

52. **Time-Series Database**: A database optimized for the storage and retrieval of time-stamped data points, commonly used in applications like IoT (Internet of Things) and financial analysis.

53. **Database Clustering**: The practice of grouping multiple database servers together to provide high availability, load balancing, and failover capabilities.

54. **Data Lake**: A large-scale, centralized repository that stores raw data from various sources in its native format for later analysis, often associated with big data architectures.

55. **Change Data Capture (CDC)**: A technique used to track and capture changes made to data in a database, enabling real-time data replication and synchronization.

56. **Data Governance**: A set of policies, procedures, and practices that ensure data quality, security, and compliance within an organization.

57. **Data Masking**: A data security technique that involves replacing sensitive data with fictional or anonymized values to protect privacy and confidentiality.

58. **Data Warehouse Automation (DWA)**: The use of software tools and processes to automate the design, construction, and management of data warehouses.

59. **Database Partitioning**: The division of a large database into smaller, more manageable partitions or segments to improve performance and scalability.

60. **Data Compression**: The process of reducing the storage space required for data in a database through various techniques, such as dictionary encoding and run-length encoding.

61. **Hadoop**: An open-source framework for distributed storage and processing of large datasets, commonly used in conjunction with NoSQL databases for big data analytics.

62. **Data Center**: A facility used to house and manage the servers, storage, and networking equipment that support database and application infrastructure.

63. **Blockchain Smart Contracts**: Self-executing contracts with the terms and conditions of an agreement directly written into code on a blockchain, automating contract enforcement.

64. **Temporal Database**: A database that stores data along with timestamps, enabling the tracking of data changes and historical data analysis.

65. **Geospatial Index**: A database index that facilitates efficient spatial queries and geospatial data retrieval, often used in GIS and mapping applications.

66. **Document-Oriented Database**: A type of NoSQL database that stores, retrieves, and manages data in the form of documents, such as JSON or XML.

67. **CAP Theorem**: A concept in distributed database systems that states that it is impossible to simultaneously achieve full consistency, availability, and partition tolerance in a distributed system.

68. **Blockchain Consensus Algorithms**: Mechanisms used in blockchain networks to achieve agreement among nodes on the validity of transactions, such as Proof of Work (PoW) and Proof of Stake (PoS).

69. **Data Archiving**: The practice of moving less frequently accessed or historical data to a separate storage location to free up resources in the primary database.

70. **Database Audit**: The process of monitoring and recording database activity, such as user logins, data changes, and security events, for compliance and security purposes.

71. **Database Firewall**: A security appliance or software that monitors and filters database traffic to protect against unauthorized access and SQL injection attacks.

72. **Data Lakehouse**: An architectural approach that combines the benefits of a data lake and a data warehouse, aiming to provide flexibility, scalability, and structured data management.

73. **Database as a Service (DBaaS)**: A cloud computing model that provides database management and hosting as a scalable and on-demand service, reducing the need for on-premises infrastructure.

74. **Polyglot Persistence**: The practice of using multiple types of databases (e.g., relational, NoSQL) to handle different data storage and processing needs within an application.

75. **Data Anonymization**: The process of removing or disguising personally identifiable information (PII) from a database to protect privacy while retaining data utility.
